{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, re, time, random, csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings for headers:\n",
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'Access-Control-Request-Headers': 'content-type',\n",
    "    'Accept': '*/*',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing NYC zips. I am excluding those I've already run: 10011, 10012, 10013, 10026, 10033, 10128, 11205\n",
    "myzips = ['10001','10002','10003','10004','10005','10006','10007','10009','10010',                 \\\n",
    "                  '10014','10016','10017','10018','10019','10020','10021','10022','10023','10024', \\\n",
    "          '10025',        '10027','10028','10029','10030','10031','10032',        '10034','10035', \\\n",
    "          '10036','10037','10038','10039','10040','10044','10045','10065','10115','10119',         \\\n",
    "          '10154','10278','10280','10301','10302','10303','10304','10305','10306','10307','10309', \\\n",
    "          '10310','10312','10314','10451','10452','10453','10454','10455','10456','10457','10458', \\\n",
    "          '10459','10460','10461','10462','10463','10464','10465','10466','10467','10468','10469', \\\n",
    "          '10471','10472','10473','10474','10475','10514','10543','10553','10573','10701','10705', \\\n",
    "          '10911','10965','10977','11001','11021','11050','11101','11102','11103','11104','11105', \\\n",
    "          '11106','11111','11112','11201','11202','11203','11204',        '11206','11207','11208', \\\n",
    "          '11209','11210','11211','11212','11213','11214','11215','11216','11217','11218','11219', \\\n",
    "          '11220','11221','11222','11223','11224','11225','11226','11228','11229','11230','11231', \\\n",
    "          '11232','11233','11234','11235','11236','11237','11238','11239','11252','11354','11355', \\\n",
    "          '11356','11357','11358','11360','11361','11362','11364','11365','11366','11367','11368', \\\n",
    "          '11369','11370','11371','11372','11373','11374','11375','11377','11378','11379','11385', \\\n",
    "          '11411','11412','11413','11414','11415','11416','11417','11418','11419','11420','11421', \\\n",
    "          '11422','11423','11424','11425','11426','11427','11428','11429','11430','11431','11432', \\\n",
    "          '11433','11434','11435','11436','11439','11451','11471','11510','11548','11566','11577', \\\n",
    "          '11580','11598','11629','11691','11692','11693','11694','11695','11731','11798','11968', \\\n",
    "          '12423','12428','12435','12458','12466','12473','12528','12701','12733','12734','12737', \\\n",
    "          '12750','12751','12754','12758','12759','12763','12764','12768','12779','12783','12786', \\\n",
    "          '12788','12789','13731','16091','20459']\n",
    "len(myzips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myzips = ['12754']\n",
    "len(myzips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myzips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting zipcode 12754\n",
      "Number of group urls for this zip is: 1\n",
      "Total number of therapists in this zip is: 15\n",
      "Finished url 1\n",
      "Finished url 2\n",
      "Finished url 3\n",
      "Finished url 4\n",
      "Finished url 5\n",
      "Finished url 6\n",
      "Finished url 7\n",
      "Finished url 8\n",
      "Finished url 9\n",
      "Finished url 10\n",
      "Finished url 11\n",
      "Finished url 12\n",
      "Finished url 13\n",
      "Finished url 14\n",
      "Finished url 15\n"
     ]
    }
   ],
   "source": [
    "# Actual web scraping - scraping Psychology Today for these zipcodes:\n",
    "# But first I need to run the function I've defined (see below)\n",
    "from single_url_scrape import scrape_single_page\n",
    "for myzip in myzips:\n",
    "    \n",
    "    print('Starting zipcode ' + myzip)\n",
    "    # Defining urls we are starting with and will be using\n",
    "    starturl = 'https://therapists.psychologytoday.com/rms/prof_results.php?search=' + myzip\n",
    "    longurl = 'https://therapists.psychologytoday.com/rms/prof_results.php?search=' + myzip + '&rec_next='\n",
    "    \n",
    "    # Beautifulsoup of the start url\n",
    "    response_zip = requests.get(starturl, headers=headers).text\n",
    "    soup_zip = BeautifulSoup(response_zip, 'html.parser')\n",
    "\n",
    "    # Manually construct the list of urls for ALL available search result pages in that tip - 20 therapists per page:\n",
    "    pages_url_list = []\n",
    "    i = 1\n",
    "    while True:\n",
    "        myurl = longurl + str(i)\n",
    "        response = requests.get(myurl, headers=headers).text\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        alert = soup.find('div', attrs={'class': 'alert-alert'})\n",
    "        if alert is None:\n",
    "            i = i + 20\n",
    "            pages_url_list.append(myurl)\n",
    "            time.sleep(random.randint(0,1))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print('Number of group urls for this zip is: ' + str(len(pages_url_list)))\n",
    "    if len(pages_url_list) == 0:\n",
    "        continue\n",
    "\n",
    "    # Loop through all urls found above (for pages of therapist search for that zip) and \n",
    "    # create a list of urls - one for each psychotherapist:\n",
    "\n",
    "    therapist_urls = []\n",
    "    for url in pages_url_list:\n",
    "        response = requests.get(url, headers = headers).text\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        therapists = soup.find_all('a', {'class': \"result-name\"})\n",
    "        for i in range(len(therapists)):\n",
    "            therapist_urls.append(therapists[i][\"href\"])\n",
    "            time.sleep(random.randint(0,1))\n",
    "    print('Total number of therapists in this zip is: ' + str(len(therapist_urls)))\n",
    "    \n",
    "    # Now that we have the list of individual therapists' urls, we can scrape that zipcode:\n",
    "\n",
    "    bigdf = pd.DataFrame()\n",
    "    for index, url in enumerate(therapist_urls):\n",
    "        response_one = requests.get(url, headers = headers).text\n",
    "        one_therap = BeautifulSoup(response_one, 'html.parser')\n",
    "        if one_therap.find('h1', {'itemprop': 'name'}) == None:\n",
    "            continue\n",
    "        smalldf = scrape_single_page(one_therap, myzip)\n",
    "        bigdf = bigdf.append(smalldf)\n",
    "        # Random sleep to avoid getting banned from the server\n",
    "        time.sleep(random.randint(0, 1))\n",
    "        # Log the progress\n",
    "        print('Finished url ' + str(index + 1))\n",
    "\n",
    "    # Drop duplicate rows in case they are there:\n",
    "    bigdf = bigdf.drop_duplicates()\n",
    "    # Save big data frame as a csv file\n",
    "    bigdf.to_csv((\"zip_\" + myzip + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function parses the Beautifulsoup of one single url (therapist)\n",
    "\n",
    "def scrape_single_page(one_therap, zipcode):\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    # Therapist name - just for checking purposes:\n",
    "    therap_name = one_therap.find('h1', {'itemprop': 'name'}).text.strip()\n",
    "    \n",
    "    # Write down the zip defined above:\n",
    "    zipcode = zipcode\n",
    "    \n",
    "    # Grab qualifications of one therapist\n",
    "    qualif_div = one_therap.find('div', {'class': \"profile-qualifications\"})\n",
    "    allqualif = qualif_div.find_all('li')   # they are saved in 'li'\n",
    "\n",
    "    # Collecting Years in Practice:\n",
    "    years_check = [i.text.split('\\n')[2].strip() for i in allqualif if \"Years in Practice\" in i.text]\n",
    "    if len(years_check) > 0:\n",
    "        years_in_practice = years_check[0].replace(' Years', '')\n",
    "    else:\n",
    "        years_in_practice = ''\n",
    "    \n",
    "    # Collecting School:\n",
    "    school_check = [i.text.split('\\n')[2].strip() for i in allqualif if \"School\" in i.text]\n",
    "    if len(school_check) > 0:\n",
    "        school = school_check[0]\n",
    "        school = re.sub(r'\\([^)]*\\)', '', school).strip()\n",
    "        school = school.replace(', ', '_')\n",
    "        school = school.replace(',', '_')\n",
    "        school = school.replace('/ ', '_')\n",
    "        school = school.replace('/', '_')\n",
    "        school = school.replace(' - ', '_')\n",
    "        school = school.replace(': ', '_')\n",
    "        school = school.replace(':', '_')\n",
    "        school = school.replace(' :', '_')\n",
    "        school = school.replace('&', 'and')\n",
    "        school = school.replace(' ', '_').lower()\n",
    "        school = 'uni_' + school\n",
    "    else:\n",
    "        school = ''\n",
    "    \n",
    "    # Collecting Year Graduated:\n",
    "    yeargrad_check = [i.text.split('\\n')[2].strip() for i in allqualif if \"Year Graduated\" in i.text]\n",
    "    if len(yeargrad_check) > 0:\n",
    "        yeargrad = int(yeargrad_check[0])\n",
    "    else:\n",
    "        yeargrad = ''\n",
    "    \n",
    "    # Collecting Board Certification:\n",
    "    board_check = [i.text.split('\\n')[2].strip() for i in allqualif if \"Board Certification\" in i.text]\n",
    "    if len(board_check) > 0:\n",
    "        board_certif = 1\n",
    "    else:\n",
    "        board_certif = 0\n",
    "    \n",
    "    # Collecting License number & State:\n",
    "    license_check = [i.text.split('\\n')[2].strip() for i in allqualif if \"License No. and State\" in i.text]\n",
    "    if len(license_check) > 0:\n",
    "        license = license_check[0].split()[0]\n",
    "        license_state = \" \".join(license_check[0].split()[1:])\n",
    "    else:\n",
    "        license = ''\n",
    "        license_state = ''\n",
    "    \n",
    "    # Grabbing all his/her titles:\n",
    "    titles = one_therap.find('div', {'class': \"profile-title\"}).text.strip()\n",
    "    titles = titles.replace(\"\\n\", \"\")\n",
    "    titles = titles.replace('/', '_').strip()\n",
    "    titles = titles.replace(', ', ',')\n",
    "    titles = titles.replace('-', '_')\n",
    "    titles = titles.replace(' ', '_')\n",
    "    titles = titles.replace(',', ', ')\n",
    "    \n",
    "    # Grabbing financial info:\n",
    "    finances = one_therap.find('div', {'class': \"profile-finances\"})\n",
    "    if finances == None:\n",
    "        per_session = ''\n",
    "        sliding = ''\n",
    "        insurance_yes = ''\n",
    "        payment_methods = 'xNotListed'\n",
    "    else:\n",
    "        # Grabbing several financial metrics:\n",
    "        lis = finances.find_all('li')\n",
    "        \n",
    "        # Grab cost per session:\n",
    "        persession_check = [i.text.split(': ')[1].strip() for i in lis if \"Avg Cost (per session)\" in i.text]\n",
    "        if len(persession_check) > 0:\n",
    "            per_session = persession_check[0].replace('$', '')\n",
    "            per_session = per_session.replace(' ', '')\n",
    "        else:\n",
    "            per_session = ''\n",
    "            \n",
    "        # Grab sliding scale (1 = yes, 0 = no):\n",
    "        sliding_check = [i.text for i in lis if 'Sliding Scale' in i.text]\n",
    "        if sliding_check != None:\n",
    "            sliding_check = \" \".join(sliding_check)\n",
    "            if 'Sliding Scale: Yes' in sliding_check:\n",
    "                sliding = 1\n",
    "            else:\n",
    "                sliding = 0\n",
    "        else:\n",
    "            sliding = 0\n",
    "        \n",
    "        # Grab Accepts Insurance - yes or no:\n",
    "        insur_check = [i.text.split(': ')[1].strip() for i in lis if \"Accepts Insurance\" in i.text]\n",
    "        if len(insur_check) > 0:\n",
    "            insurance_yes = 1   # insur_check[0]\n",
    "        else:\n",
    "            insurance_yes = 0    # ''\n",
    "        \n",
    "        # Grab Methods of Payment:\n",
    "        # payment_check = [i.text for i in lis if \"Accepted Payment Methods\" in i.text]\n",
    "        payment_check = [i.text.split('\\n')[2].strip().split(',') for i in lis if \"Accepted Payment Methods\" in i.text]\n",
    "        if len(payment_check) == 0:\n",
    "            payment_methods = 'xNotListed'\n",
    "        else:\n",
    "            payment_methods = [item for sublist in payment_check for item in sublist]\n",
    "            for index, item in enumerate(payment_methods):\n",
    "                out = item.strip().lower()\n",
    "                out = out.replace(\" \", \"_\")\n",
    "                out = 'paym_' + out\n",
    "                payment_methods[index] = out\n",
    "            payment_methods = ', '.join(payment_methods)\n",
    "        \n",
    "    # Grab insurance plans s/he accepts:\n",
    "    long = one_therap.find_all('div', {'class': \"col-xs-12 col-sm-6 col-md-6 col-lg-6 col-tight-right\"})\n",
    "    if len(long) > 0:\n",
    "        result = []\n",
    "        for i in long:\n",
    "            result.extend(i.find_all('li'))\n",
    "        plans = []\n",
    "        for i in range(len(result)):\n",
    "            out = result[i].text\n",
    "            out = re.sub(r'\\([^)]*\\)', '', out)\n",
    "            out = out.replace(' ', '_')\n",
    "            out = out.replace('|', '_')\n",
    "            out = out.replace('\\|', '_')\n",
    "            out = out.replace('&', 'and')\n",
    "            out = out.replace('-', '_')\n",
    "            out = out.replace('/', '_')\n",
    "            out = 'ins_' + out\n",
    "            plans.append(out)\n",
    "        plans = ', '.join(plans).lower()\n",
    "    else:\n",
    "        plans = 'xNotListed'\n",
    "\n",
    "    # Grab specialties:\n",
    "    spec_check = one_therap.find_all('li', {'class': \"highlight\"})\n",
    "    if len(spec_check) == 0:\n",
    "        specialties = 'xNotListed'\n",
    "    else:\n",
    "        specialties = []\n",
    "        for i in range(len(spec_check)):\n",
    "            out = spec_check[i].text.replace(', ', '_')\n",
    "            out = out.replace(' ', '_').lower()\n",
    "            out = 'spec_' + out\n",
    "            specialties.append(out)\n",
    "        specialties = \", \".join(specialties)\n",
    "    \n",
    "    # Grab issues:\n",
    "    issues_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-6 col-lg-6'})\n",
    "    if len(issues_check) == 0:\n",
    "        issues = 'xNotListed'\n",
    "    else:\n",
    "        result = []\n",
    "        for i in issues_check:\n",
    "            result.extend(i.find_all('li'))\n",
    "        issues = []\n",
    "        for i in range(len(result)):\n",
    "            out = result[i].text.strip()\n",
    "            out = re.sub(r'\\([^)]*\\)', '', out)\n",
    "            out = re.sub(r' $', '', out)\n",
    "            out = out.replace(', ', '_')\n",
    "            out = out.replace('-', '_')\n",
    "            out = out.replace('\\'', '')\n",
    "            out = out.replace(' ', '_').lower()\n",
    "            out = 'iss_' + out\n",
    "            issues.append(out)\n",
    "        issues = \", \".join(issues)\n",
    "    \n",
    "    # Grab preferred ages:\n",
    "    \n",
    "    # In rare cases when no specialties were selected, we need to grab the 3rd, not the 4th element:\n",
    "    if specialties == '':\n",
    "        age_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-12 col-lg-12'})[2]\n",
    "    else:\n",
    "        age_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-12 col-lg-12'})[3]\n",
    "    \n",
    "    age_lis = age_check.find_all('li')\n",
    "    ages = []\n",
    "    for i in age_lis:\n",
    "        ages.append(i.text)\n",
    "    \n",
    "    # Creating values for age categories:\n",
    "    \n",
    "    if \"Toddlers / Preschoolers (0 to 6)\" in ages:      # Age - toddlers:\n",
    "        age_toddlers = 1\n",
    "    else:\n",
    "        age_toddlers = 0\n",
    "    \n",
    "    if \"Children (6 to 10)\" in ages:                # Age - children:\n",
    "        age_children = 1\n",
    "    else:\n",
    "        age_children = 0\n",
    "    \n",
    "    if \"Preteens / Tweens (11 to 13)\" in ages:      # Age - preteens:\n",
    "        age_preteens = 1\n",
    "    else:\n",
    "        age_preteens = 0\n",
    "    \n",
    "    if \"Adolescents / Teenagers (14 to 19)\" in ages:      # Age - teens:\n",
    "        age_teens = 1\n",
    "    else:\n",
    "        age_teens = 0\n",
    "    \n",
    "    if \"Adults\" in ages:              # Age - adults:\n",
    "        age_adults = 1\n",
    "    else:\n",
    "        age_adults = 0\n",
    "    \n",
    "    if \"Elders (65+)\" in ages:        # Age - elders:\n",
    "        age_elders = 1\n",
    "    else:\n",
    "        age_elders = 0\n",
    "    \n",
    "    # Sum of ages:\n",
    "    agesum = age_toddlers+age_children+age_preteens+age_teens+age_adults+age_elders\n",
    "    \n",
    "    \n",
    "    # Grab subcategories:\n",
    "    \n",
    "    if specialties == '' and agesum == 0:\n",
    "        subcat_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-12 col-lg-12'})[2]\n",
    "    elif specialties == '' and agesum > 0:\n",
    "        subcat_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-12 col-lg-12'})[3]\n",
    "    elif specialties != '' and agesum == 0:\n",
    "        subcat_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-12 col-lg-12'})[3]\n",
    "    else:\n",
    "        subcat_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-12 col-lg-12'})[4]\n",
    "    \n",
    "    subcat_lis = subcat_check.find_all('li')\n",
    "    subcats = []\n",
    "    for i in subcat_lis:\n",
    "        subcats.append(i.text)\n",
    "    \n",
    "    if \"Aviation Professionals\" in subcats:  # Subcategories - Aviation:\n",
    "        sub_pilots = 1\n",
    "    else:\n",
    "        sub_pilots = 0\n",
    "    \n",
    "    if \"Bisexual Clients\" in subcats:  # Subcategories - HIV:\n",
    "        sub_bisexuals = 1\n",
    "    else:\n",
    "        sub_bisexuals = 0\n",
    "    \n",
    "    if \"Cancer\" in subcats:      # Subcategories - Cancer:\n",
    "        sub_cancer = 1\n",
    "    else:\n",
    "        sub_cancer = 0\n",
    "    \n",
    "    if \"Gay Clients\" in subcats:  # Subcategories - Gays:\n",
    "        sub_gays = 1\n",
    "    else:\n",
    "        sub_gays = 0\n",
    "    \n",
    "    if \"HIV / AIDS Clients\" in subcats:  # Subcategories - HIV:\n",
    "        sub_hiv = 1\n",
    "    else:\n",
    "        sub_hiv = 0\n",
    "    \n",
    "    if \"Heterosexual Clients\" in subcats:  # Subcategories - Heterosexual:\n",
    "        sub_heteros = 1\n",
    "    else:\n",
    "        sub_heteros = 0\n",
    "    \n",
    "    if \"Lesbian Clients\" in subcats:   # Subcategories - Lesbian:\n",
    "        sub_lesbians = 1\n",
    "    else:\n",
    "        sub_lesbians = 0\n",
    "    \n",
    "    if \"Transgender Clients\" in subcats:  # Subcategories - Transgender:\n",
    "        sub_transgender = 1\n",
    "    else:\n",
    "        sub_transgender = 0\n",
    "    \n",
    "    if \"Veterans\" in subcats:   # Subcategories - Veterans:\n",
    "        sub_veterans = 1\n",
    "    else:\n",
    "        sub_veterans = 0\n",
    "    \n",
    "    # Creating 2 additional sums\n",
    "    subsum = sub_pilots+sub_bisexuals+sub_cancer+sub_gays+sub_hiv+sub_heteros+sub_lesbians+sub_transgender+sub_veterans\n",
    "    age_and_sub_sum = agesum + subsum\n",
    "    \n",
    "    # Check if Treatment Approach is even there:\n",
    "    approach_check = one_therap.find_all('h2')\n",
    "    asalist = []\n",
    "    count = 1\n",
    "    for i in approach_check:\n",
    "        try:\n",
    "            out = i.text\n",
    "        except:\n",
    "            print(\"some error\")\n",
    "        asalist.append(out)\n",
    "    if 'Treatment Approach' in asalist:\n",
    "    \n",
    "        # Grab treatment approaches:\n",
    "        approach_check = one_therap.find_all('div', {'class': 'col-xs-12 col-sm-12 col-md-12 col-lg-12'})\n",
    "        if specialties == '' and age_and_sub_sum == 0:           # if all 3 are not there\n",
    "            approach_check = approach_check[2]\n",
    "        elif specialties != '' and age_and_sub_sum == 0:         # if only 2 out of 3 are not there\n",
    "            approach_check = approach_check[3]\n",
    "        elif specialties == '' and agesum == 0 and subsum > 0:\n",
    "            approach_check = approach_check[3]\n",
    "        elif specialties == '' and agesum > 0 and subsum == 0:\n",
    "            approach_check = approach_check[3]\n",
    "        elif specialties != '' and agesum == 0  and subsum > 0:  # if only 1 out of 3 is not there\n",
    "            approach_check = approach_check[4]\n",
    "        elif specialties != '' and agesum > 0  and subsum == 0:\n",
    "            approach_check = approach_check[4]\n",
    "        elif specialties == '' and agesum > 0  and subsum > 0:\n",
    "            approach_check = approach_check[4]\n",
    "        else:\n",
    "            approach_check = approach_check[5]\n",
    "\n",
    "        approach_lis = approach_check.find_all('li')\n",
    "        approaches = []\n",
    "        for i in approach_lis:\n",
    "            approaches.append(i.text)\n",
    "\n",
    "        for i in range(len(approaches)):\n",
    "            out = approaches[i]\n",
    "            out = out.replace(' / ', '_')\n",
    "            out = re.sub(r'\\([^)]*\\)', '', out)\n",
    "            out = re.sub(r' $', '', out)\n",
    "            out = out.strip().replace('-', '_')\n",
    "            out = out.replace(' / ', '_')\n",
    "            out = out.replace('/ ', '_')\n",
    "            out = out.replace('/', '_')\n",
    "            out = out.replace('\\'', '')\n",
    "            out = out.replace(' ', '_').lower()\n",
    "            out = 'appr_' + out\n",
    "            approaches[i] = out\n",
    "        approaches = ', '.join(approaches)\n",
    "    else:\n",
    "        approaches = 'xNotListed'\n",
    "    \n",
    "    # Grab ethnicities, languages, religious orientation:\n",
    "    spec_subcat = one_therap.find_all('div', {'class': \"spec-subcat\"})\n",
    "    list_of_children = [None] * len(spec_subcat)\n",
    "    for i in range(len(spec_subcat)):\n",
    "        list_of_children[i] = spec_subcat[i].findChildren()\n",
    "    whatsthere = []\n",
    "    for i in range(len(list_of_children)):\n",
    "        for z in range(len(list_of_children[i])):\n",
    "            for t in [text for text in list_of_children[i][z].stripped_strings]:\n",
    "                whatsthere.append(t)\n",
    "    for i in range(len(whatsthere)):\n",
    "        whatsthere[i] = whatsthere[i].replace(',', '')\n",
    "\n",
    "    # Finding indexes for Ethnicity, Languages, and Religious Orientation:\n",
    "    if one_therap.find_all(\"strong\", string=\"Ethnicity:\") == []:\n",
    "        ethnicities = 'X'\n",
    "        index_ethnic = ''\n",
    "    else:\n",
    "        index_ethnic = whatsthere.index(\"Ethnicity:\")\n",
    "\n",
    "    if one_therap.find_all(\"strong\", string=\"Alternative Languages:\") == []:\n",
    "        languages = 'X'\n",
    "        index_languages = ''\n",
    "    else:\n",
    "        index_languages = whatsthere.index(\"Alternative Languages:\")\n",
    "    index_languages\n",
    "    if one_therap.find_all(\"strong\", string=\"Religious Orientation:\") == []:\n",
    "        religion = 'X'\n",
    "        index_religion = ''\n",
    "    else:\n",
    "        index_religion = whatsthere.index(\"Religious Orientation:\")\n",
    "\n",
    "    # Grabbing applicable info - depending on what's there and what's not:\n",
    "    if index_ethnic != '' and index_languages != '' and index_religion != '':  # if all 3 are present\n",
    "        ethnicities = whatsthere[(index_ethnic+1):index_languages]\n",
    "        languages = whatsthere[(index_languages+1):index_religion]\n",
    "        religion = whatsthere[(index_religion+1):len(whatsthere)]\n",
    "\n",
    "    if index_ethnic == '' and index_languages != '' and index_religion != '':   # if only languages and religion present\n",
    "        languages = whatsthere[(index_languages+1):index_religion]\n",
    "        religion = whatsthere[(index_religion+1):len(whatsthere)]\n",
    "\n",
    "    if index_ethnic != '' and index_languages == '' and index_religion != '':   # if only ethnicities and religion present\n",
    "        ethnicities = whatsthere[(index_ethnic+1):index_religion]\n",
    "        religion = whatsthere[(index_religion+1):len(whatsthere)]\n",
    "\n",
    "    if index_ethnic != '' and index_languages != '' and index_religion == '':   # if only ethnicities and languages present\n",
    "        ethnicities = whatsthere[(index_ethnic+1):index_languages]\n",
    "        languages = whatsthere[(index_languages+1):len(whatsthere)]\n",
    "\n",
    "    if index_ethnic != '' and index_languages == '' and index_religion != '':   # if only ethnicities and religion present\n",
    "        ethnicities = whatsthere[(index_ethnic+1):index_religion]\n",
    "        religion = whatsthere[(index_religion+1):len(whatsthere)]\n",
    "\n",
    "    if index_ethnic != '' and index_languages == '' and index_religion == '':   # if only ethnicities present\n",
    "        ethnicities = whatsthere[(index_ethnic+1):len(whatsthere)]\n",
    "\n",
    "    if index_ethnic == '' and index_languages != '' and index_religion == '':   # if only languages present\n",
    "        languages = whatsthere[(index_languages+1):len(whatsthere)]\n",
    "\n",
    "    if index_ethnic == '' and index_languages == '' and index_religion != '':   # if only religion present\n",
    "        religion = whatsthere[(index_religion+1):len(whatsthere)]\n",
    "\n",
    "    ethnicities = \", \".join(ethnicities)\n",
    "    languages = \", \".join(languages)\n",
    "    religion = \", \".join(religion)\n",
    "    \n",
    "    dic['atherapist'] = [therap_name]\n",
    "    dic['zipcode'] = [zipcode]\n",
    "    dic['years_in_practice'] = [years_in_practice]  #.encode('utf-8', 'ignore')\n",
    "    dic['school'] = [school]\n",
    "    dic['yeargrad'] = [yeargrad]\n",
    "    dic['board_certif'] = [board_certif]\n",
    "    dic['license'] = [license]\n",
    "    dic['license_state'] = [license_state]\n",
    "    dic['titles'] = [titles]\n",
    "    dic['per_session'] = [per_session]\n",
    "    dic['per_session'] = [per_session]\n",
    "    dic['sliding'] = [sliding]\n",
    "    dic['insurance_yes'] = [insurance_yes]\n",
    "    dic['payment_methods'] = [payment_methods]\n",
    "    dic['plans'] = [plans]\n",
    "    dic['specialties'] = [specialties]\n",
    "    dic['issues'] = [issues]\n",
    "    dic['age_toddlers'] = [age_toddlers]\n",
    "    dic['age_children'] = [age_children]\n",
    "    dic['age_preteens'] = [age_preteens]\n",
    "    dic['age_teens'] = [age_teens]\n",
    "    dic['age_adults'] = [age_adults]\n",
    "    dic['age_elders'] = [age_elders]\n",
    "    dic['sub_pilots'] = [sub_pilots]\n",
    "    dic['sub_bisexuals'] = [sub_bisexuals]\n",
    "    dic['sub_cancer'] = [sub_cancer]\n",
    "    dic['sub_gays'] = [sub_gays]\n",
    "    dic['sub_hiv'] = [sub_hiv]\n",
    "    dic['sub_heteros'] = [sub_heteros]\n",
    "    dic['sub_lesbians'] = [sub_lesbians]\n",
    "    dic['sub_transgender'] = [sub_transgender]\n",
    "    dic['sub_veterans'] = [sub_veterans]\n",
    "    dic['approaches'] = [approaches]\n",
    "    dic['client_ethnicities'] = [ethnicities]\n",
    "    dic['client_languages'] = [languages]\n",
    "    dic['client_religion'] = [religion]\n",
    "    return pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing my two functions for webscraping:\n",
    "from single_url_scrape import scrape_single_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing NYC zips. I am excluding those I've already run:    \n",
    "\n",
    "myzips = ['10001','10002','10003','10004','10005','10006','10007','10009','10010','10011', '10012',\\\n",
    "          '10013','10014','10016','10017','10018','10019','10020','10021','10022','10023','10024', \\\n",
    "          '10025','10026','10027','10028','10029','10030','10031','10032','10033','10034','10035', \\\n",
    "          '10036','10037','10038','10039','10040','10044','10045','10065','10115','10119','10128', \\\n",
    "          '10154','10278','10280','10301','10302','10303','10304','10305','10306','10307','10309', \\\n",
    "          '10310','10312','10314','10451','10452','10453','10454','10455','10456','10457','10458', \\\n",
    "          '10459','10460','10461','10462','10463','10464','10465','10466','10467','10468','10469', \\\n",
    "          '10471','10472','10473','10474','10475','10514','10543','10553','10573','10701','10705', \\\n",
    "          '10911','10965','10977','11001','11021','11050','11101','11102','11103','11104','11105', \\\n",
    "          '11106','11111','11112','11201','11202','11203','11204','11205','11206','11207','11208', \\\n",
    "          '11209','11210','11211','11212','11213','11214','11215','11216','11217','11218','11219', \\\n",
    "          '11220','11221','11222','11223','11224','11225','11226','11228','11229','11230','11231', \\\n",
    "          '11232','11233','11234','11235','11236','11237','11238','11239','11252','11354','11355', \\\n",
    "          '11356','11357','11358','11360','11361','11362','11364','11365','11366','11367','11368', \\\n",
    "          '11369','11370','11371','11372','11373','11374','11375','11377','11378','11379','11385', \\\n",
    "          '11411','11412','11413','11414','11415','11416','11417','11418','11419','11420','11421', \\\n",
    "          '11422','11423','11424','11425','11426','11427','11428','11429','11430','11431','11432', \\\n",
    "          '11433','11434','11435','11436','11439','11451','11471','11510','11548','11566','11577', \\\n",
    "          '11580','11598','11629','11691','11692','11693','11694','11695','11731','11798','11968', \\\n",
    "          '12423','12428','12435','12458','12466','12473','12528','12701','12733','12734','12737', \\\n",
    "          '12750','12751','12754','12758','12759','12763','12764','12768','12779','12783','12786', \\\n",
    "          '12788','12789','13731','16091','20459']\n",
    "len(myzips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing my main function for webscraping 'myscrape' saved in 'myscrape_function.py'\n",
    "from myscrape_function import myscrape\n",
    "# That function, in turn, imports and uses function 'scrape_single_page'\n",
    "# from file 'single_url_scrape.py' that parses the BeautifulSoup of one url (one therapist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myzips = ['12754', '12758']\n",
    "len(myzips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting zipcode 12754\n",
      "Number of group urls for this zip is: 1\n",
      "Total number of therapists in this zip is: 15\n",
      "Finished url 2\n",
      "Finished url 3\n",
      "Finished url 4\n",
      "Finished url 5\n",
      "Finished url 6\n",
      "Finished url 7\n",
      "Finished url 9\n",
      "Finished url 10\n",
      "Finished url 11\n",
      "Finished url 12\n",
      "Finished url 13\n",
      "Finished url 14\n",
      "Finished url 15\n",
      "Starting zipcode 12758\n",
      "Number of group urls for this zip is: 1\n",
      "Total number of therapists in this zip is: 20\n",
      "Finished url 1\n",
      "Finished url 2\n",
      "Finished url 3\n",
      "Finished url 4\n",
      "Finished url 5\n",
      "Finished url 6\n",
      "Finished url 7\n",
      "Finished url 8\n",
      "Finished url 9\n",
      "Finished url 10\n",
      "Finished url 11\n",
      "Finished url 12\n",
      "Finished url 13\n",
      "Finished url 14\n",
      "Finished url 15\n",
      "Finished url 16\n",
      "Finished url 17\n",
      "Finished url 18\n",
      "Finished url 19\n",
      "Finished url 20\n"
     ]
    }
   ],
   "source": [
    "myscrape(myzips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
